#!/bin/bash
read -p 'enter "c" for create "d" for delete: ' IN
read -p 'enter volume: ' VOLUME
#read -p 'enter dir: ' SUB 
DIR=/portal/bricks/$VOLUME

if [ "$IN" == "c" ]; then 
read -p 'enter user: ' USER 
#read -p 'enter group: ' GROUP 
for i in gfs1 gfs2 gfs3 ; do 
ssh $i mkdir $DIR
ssh $i chown $USER:$USER $DIR
ssh $i semanage fcontext -a -t fusefs_t $DIR
ssh $i restorecon -v $DIR
ls -ldZ $DIR
done
gluster volume create $VOLUME replica 3 gfs1:$DIR gfs2:$DIR gfs3:$DIR
{% set server_ip = [] %} {% for ip in host_ip  %} {{ server_ip.append( ip ) }} {% endfor %}

gluster volume set $VOLUME auth.allow "{{ server_ip|join(',') }}"
gluster volume start $VOLUME
elif [ "$IN" == "d" ]; then
gluster volume stop $VOLUME
gluster volume delete $VOLUME
for i in gfs1 gfs2 gfs3 ; do
ssh $i rm -rf $DIR
done
else
echo "enter \"c\" for create or \"d\" for delete "
exit 0
fi

# Create pv and pvc 
PVNAME=${VOLUME}
read -p 'pvsize: ' PVSIZE
read -p 'pvuser: ' PVUSER
GLUSTER_PATH=${VOLUME}

cat >>./pv-pvc.yaml<<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${PVNAME}
  annotations:
    pv.beta.kubernetes.io/gid: "${PVUSER}" 
spec:
  capacity:
    storage: "${PVSIZE}Gi"
  accessModes: 
    - ReadWriteMany
  glusterfs:
    endpoints: glusterfs-cluster 
    path: ${GLUSTER_PATH}
    readOnly: false
  persistentVolumeReclaimPolicy: Retain

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${PVNAME}-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: "${PVSIZE}Gi"
  volumeName: "${PVNAME}"
EOF
oc create -f pv-pvc.yaml
rm -rf pv-pvc.yaml
oc get pvc $PVNAME-pvc
